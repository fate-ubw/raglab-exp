# raglab
![alt text](https://github.com/fate-ubw/raglab-exp/blob/main/figures/image.png)

# ğŸ”¨Install environment
- æœºå™¨é…ç½®ï¼špytorch:2.0.1-py3.10-cuda11.8.0-devel-ubuntu22.04
- [install miniconda](https://docs.anaconda.com/free/miniconda/index.html)

- git clone raglab
~~~bash
git clone https://github.com/fate-ubw/raglab-exp.git
~~~
- create environment from yml 
~~~bash
cd raglab-exp
conda create -f environment.yml
~~~
- install flash-attn, en_core_web_sm, punkt manually
~~~bash
pip install flash-attn==2.2
python -m spacy download en_core_web_sm
python -m nltk.downloader punkt
~~~

# ğŸ¤–Model
- raglab need llama2-7b, llama3-instruction, colbertv2.0, selfrag
~~~bash
cd raglab-exp
mkdir model
cd model
mkdir output_models
mkdir Llama-2-7b-chat-hf
mkdir Meta-Llama-3-8B-Instruct
mkdir selfrag_llama2_7b
mkdir colbertv2.0
huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --local-dir Meta-Llama-3-8B-Instruct/
huggingface-cli download meta-llama/Llama-2-7b-chat-hf --local-dir Llama-2-7b-chat-hf/
huggingface-cli download selfrag/selfrag_llama2_7b --local-dir selfrag_llama2_7b
huggingface-cli download colbert-ir/colbertv2.0 --local-dir colbertv2.0/
~~~

# ğŸ’½process wiki2023 as vector database

## download wiki2023 raw data
- current version of raglab use wiki2023 as database
- we get source wiki2023 get from [factscore](https://github.com/shmsw25/FActScore)
  - method1: url for download wiki2023:[google_drive](https://drive.google.com/file/d/1mekls6OGOKLmt7gYtHs0WGf5oTamTNat/view) 
  - method2: install throuth gdown 
  ~~~bash
  cd raglab-exp/data/retrieval/colbertv2.0_passages
  mkdir wiki2023
  pip install gdown
  gdown --id 1mekls6OGOKLmt7gYtHs0WGf5oTamTNat
  ~~~
## 10-samples test
- use 10-samples test environment
- run colbert embedding process enwiki-20230401-10samples.tsv
- ç¬¬ä¸€æ­¥ï¼šé¦–å…ˆéœ€è¦ä¿®æ”¹raglab-exp/preprocess/colbert-wiki2023-preprocess/wiki2023-10samples_tsv-2-colbert_embedding.py æ–‡ä»¶ä¸­çš„è·¯å¾„ï¼Œcolbert åœ¨ç”Ÿæˆ embedding çš„æ—¶å€™ä½¿ç”¨ç›¸å¯¹è·¯å¾„ä¼šé€ æˆå¾ˆå¤šé—®é¢˜ï¼Œå›ºå½“å‰ç‰ˆæœ¬ raglab ä½¿ç”¨ç»å¯¹è·¯å¾„
~~~bash
  # change root path
    checkpoint = '/your_root_path/raglab-exp/model/colbertv2.0'
    index_dbPath = '/your_root_path/raglab-exp/data/retrieval/colbertv2.0_embedding/wiki2023-10samples'
    collection = '/your_root_path/raglab-exp/data/retrieval/colbertv2.0_passages/wiki2023-10samples/enwiki-20230401-10samples.tsv'
~~~
- ç¬¬äºŒæ­¥ï¼šå¯åŠ¨è„šæœ¬å¼€å§‹å¤„ç†
~~~bash
cd raglab-exp
sh run/wiki2023_preprocess/2-wiki2023-10samples_tsv-2-colbert_embedding.sh
~~~
- this will take around 15mins. 
- 10-samples test ç›®æ ‡æ˜¯ä¸ºäº†éªŒè¯ç¯å¢ƒï¼Œcolbert embeddingç¬¬ä¸€æ¬¡å¤„ç†èŠ±è´¹æ—¶é—´è¾ƒå¤§ï¼Œå› ä¸ºéœ€è¦é‡æ–°ç¼–è¯‘ torch_extensionsã€‚è°ƒç”¨å¤„ç†å¥½çš„ embedding åˆ™æ— éœ€èŠ±è´¹å¾ˆé•¿æ—¶é—´
-  è‹¥æ— æŠ¥é”™å¹¶ä¸”å¯ä»¥æ‰“æ£€ç´¢åˆ°çš„æ–‡æœ¬ï¼Œåˆ™è¯´æ˜ç¯å¢ƒæ­£ç¡®
## Test Raglab with 10-samples embedding
- here we test naive rag base  10-samples embedding
- ç»è¿‡ colbert embedding çš„å¤„ç†ä¹‹åå°±å¯ä»¥å¼€å§‹è¿è¡Œ raglab ä¸­çš„ç®—æ³•äº†ï¼Œraglabé›†æˆçš„æ‰€æœ‰ç®—æ³•éƒ½åŒ…å« `interact` å’Œ `evaluation` ä¸¤ä¸ª modeï¼Œtest é˜¶æ®µå±•ç¤º `interact mode`
- ä¿®æ”¹`/workspace/raglab-exp/config/selfrag_reproduction/selfrag_reproduction-interact-short_form-adaptive_retrieval.yaml`æ–‡ä»¶ä¸­ `index_dbPath` å’Œ `text_dbPath`. æ³¨æ„ colbert å¿…é¡»ä½¿ç”¨ç»å¯¹è·¯å¾„
~~~bash
# å…¶ä»–å‚æ•°ä¸éœ€è¦ä¿®æ”¹
index_dbPath: /your_root_path/raglab-exp/data/retrieval/colbertv2.0_embedding/wiki2023-10samples
text_dbPath: /your_root_path/raglab-exp/data/retrieval/colbertv2.0_passages/wiki2023-10samples/enwiki-20230401-10samples.tsv
~~~
- run selfrag (short form & adaptive retrieval) interact mode test 10-samples embedding
- you can also run other algorithms in interaction mode
~~~bash
cd raglab-exp
sh run/rag_inference/3-selfrag_reproduction-interact-short_form-adaptive_retrieval.sh
~~~
- In raglab, all algorithms in interact mode return two variables: response and generation_track. Moreover, each algorithm has 10 queries built-in in interact mode which are sampled from benchmark
## embedding whole wiki2023
- å¦‚æœé¡ºåˆ©é€šè¿‡ 10-samples  test åˆ™å¯ä»¥è¿›è¡Œ wiki2023 çš„å¤„ç†
- preprocess `.db -> .tsv` (colbert åªèƒ½è¯»å–.tsvæ ¼å¼çš„æ–‡ä»¶)
~~~bash
cd raglab-exp
sh run/wiki2023_preprocess/3-wiki2023_db-2-tsv.sh
~~~
- `.tsv -> embedding`
~~~bash
cd raglab-exp
sh run/wiki2023_preprocess/2-wiki2023-10samples_tsv-2-colbert_embedding.sh
~~~
- attentionï¼šnranks ä¸ºè®¾å®š gpu æ•°é‡å‚æ•°ï¼Œå½“å‰nranks=8

# Fine tune llama3 & self rag 
- raglab baseline å’Œ selfrag çš„åŸºåº§æ¨¡å‹é‡‡ç”¨ `llama3-instruction-8b`ã€‚ç”±äº self rag åœ¨å¾®è°ƒé˜¶æ®µå¤šè®­ç»ƒäº†ä¸€éƒ¨åˆ†æ•°æ®ï¼Œä¸ºäº†å…¬å¹³æ¯”è¾ƒ baseline model åŒæ ·éœ€è¦è¿›è¡Œå¾®è°ƒ 
## download self rag train data
- we get the train data from [selfrag](https://github.com/AkariAsai/self-rag/tree/main)
- google drive [url](https://drive.google.com/file/d/10G_FozUV4u27EX0NjwVe-3YMUMeTwuLk/view)
- download through gdown
~~~bash
cd raglab-exp/data/train_data/
gdown --id 10G_FozUV4u27EX0NjwVe-3YMUMeTwuLk
~~~
## 10-samples test for fintune
- 10samples train dataset å·²ç»å‡†å¤‡å¥½äº†ï¼Œè¯·ç›´æ¥å¯åŠ¨ bash è„šæœ¬å¼€å§‹æµ‹è¯•
- æ³¨æ„ï¼šæµ‹è¯•è„šæœ¬ä»…å¯åŠ¨ä¸€å¼  gpu è¿›è¡Œå…¨å‚æ•°finetune 
~~~bash
cd raglab-exp
sh run/rag_train/script_finetune-llama3-baseline-full_weight-10samples.sh
~~~
- è‹¥ 10-samples testé¡ºåˆ©é€šè¿‡ï¼Œåˆ™è¯æ˜ç¯å¢ƒæ— è¯¯
## finetune self rag 8b
- finetune directly
~~~bash
cd raglab-exp
sh run/rag_train/script_finetune-selfrag_8b-full_weight.sh
~~~
## finetune llama3-7b-instruction as baseline
- preprocess train data. Train data for baseline model need remove special tokens.
~~~bash
cd raglab-exp
sh run/traindataset_preprocess/selfrag_traindata-remove_special_tokens.sh
~~~
- then you will get baseline train_data without special token (what is specal token? Anawer: special tokens is a concept proposed by SelfRAG)
- finttune baseline ues processed data
~~~bash
 sh run/rag_train/script_finetune-llama3-baseline-full_weight.sh
~~~
# Inference exp
- å®Œæˆä¸Šè¿°å®éªŒä¹‹åå³å¯å¼€å§‹æ‰€æœ‰ç®—æ³•çš„æ¨ç†
- baseline çš„ temperature ä»¥åŠ top_p å‚æ•°è¿˜éœ€è¦è®¨è®º
- å› ä¸ºéœ€è¦æ¨ç†çš„è„šæœ¬éå¸¸å¤šï¼Œéœ€è¦ä½¿ç”¨ gpu è°ƒåº¦ç®—æ³•[simple_gpu_scheduler](https://github.com/ExpectationMax/simple_gpu_scheduler)æ¥å®ç°ä¸€ä¸ªè„šæœ¬è°ƒç”¨æ‰€æœ‰çš„ bash è„šæœ¬è¿›è¡Œæ‰€æœ‰çš„å®éªŒ