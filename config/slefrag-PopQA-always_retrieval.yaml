# GPU config
num_gpu: 2
# eval config
eval_datapath: /home/wyd/raglab-exp/data/eval_datasets/popqa_longtail_w_gs.jsonl
task: PopQA
output_dir: /home/wyd/raglab-exp/1-eval_output
# llm config
llm_path: /home/wyd/model/selfrag_llama2_7b
generate_maxlength: 100
use_vllm:
# retrieval common config
retrieval_name: contriever
index_dbPath: /home/wyd/data/5-contriever/wikipedia_embeddings/*
text_dbPath: /home/wyd/data/5-contriever/psgs_w100.tsv
retriever_modelPath: /home/wyd/model/contriever-msmarco
n_docs: 10
# colbert config
nbits: 2
# contriever config
projection_size: 768
indexing_batch_size: 1000000
n_bits: 8
n_subquantizers: 0
per_gpu_batch_size: 64 #Batch size for question encoding 为什么 question 的时候是 64 维度，可能得看一下论文
question_maxlength: 512 # Maximum number of tokens in a question
# self rag 
download_dir: ".cache"
world_size: 1
dtype: "half"
threshold: 0.2  
use_seqscore: True
use_groundness: True
use_utility: True
beam_width: 2
max_depth: 2
w_rel: 1.0
w_sup: 1.0
w_use: 0.5
retrieval_mode: "always_retrieval"
show_specialtokens: True
realtime_retrieval: True

